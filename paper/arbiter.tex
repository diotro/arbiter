\documentclass[sigconf]{acmart}
% \usepackage{aaai20}
% \usepackage{times}
% \usepackage{helvet}
% \usepackage{courier}
\usepackage{graphicx}
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
% Add additional packages here, but check
% the list of disallowed packages
% (including, but not limited to
% authblk, caption, CJK, float, fullpage, geometry,
% hyperref, layout, nameref, natbib, savetrees,
% setspace, titlesec, tocbibind, ulem)
% and illegal commands provided in the
% common formatting errors document
% included in the  Author Kit before doing so.
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}

% \newcommand{\citep}[1]{\cite{#1}}
% \newcommand{\citet}[1]{\citeauthor{#1} \shortcite{#1}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}
\fancyhead{}

% PDFINFO
% You are required to complete the following
% for pass-through to the PDF.
% No LaTeX commands of any kind may be
% entered. The parentheses and spaces
% are an integral part of the
% pdfinfo script and must not
%
\pdfinfo{
		/Title (Arbiter: A Domain-Specific Language for Ethical Machine Learning)
		/Author (Julian Zucker and Myraeka d'Leeuwen)
}
\title{Arbiter: A Domain-Specific Language \\for Ethical Machine Learning}
\author{Julian Zucker}
\affiliation{%
    \institution{Northeastern University}
}
\email{julian.zucker@gmail.com}

\author{Myraeka d'Leeuwen}
\affiliation{%
    \institution{Northeastern University}
}
\email{dleeuwen.m@husky.neu.edu}

\renewcommand{\shortauthors}{Zucker and d'Leeuwen}


%
% Section Numbers
% Uncomment if you want to use section numbers
% and change the 0 to a 1 or 2
% \setcounter{secnumdepth}{1}
\copyrightyear{2020}
\acmYear{2020}
\setcopyright{acmlicensed}
\acmConference[AIES '20]{Proceedings of the 2020 AAAI/ACM Conference on AI, Ethics, and Society}{February 7--8, 2020}{New York, NY, USA}
\acmBooktitle{Proceedings of the 2020 AAAI/ACM Conference on AI, Ethics, and Society (AIES '20), February 7--8, 2020, New York, NY, USA}
\acmPrice{15.00}
\acmDOI{10.1145/3375627.3375858}
\acmISBN{978-1-4503-7110-0/20/02}


\begin{document}
\begin{abstract}
The widespread deployment of machine learning models in high-stakes decision making scenarios requires a code of ethics for machine learning practitioners. We identify four of the primary components required for the ethical practice of machine learning: transparency, fairness, accountability, and reproducibility. We introduce Arbiter, a domain-specific programming language for machine learning practitioners that is designed for ethical machine learning. Arbiter provides a notation for recording how machine learning models will be trained, and we show how this notation can encourage the four described components of ethical machine learning.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010257</concept_id>
<concept_desc>Computing methodologies~Machine learning</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011050.10011017</concept_id>
<concept_desc>Software and its engineering~Domain specific languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[500]{Software and its engineering~Domain specific languages}

\keywords{domain-specific languages, ethical machine learning}


\maketitle

\section{Introduction}
In this paper, we discuss what ethical machine learning is, demonstrate what a domain-specific programming language for ethical machine learning could look like, and demonstrate how that language will aid in the practice of ethical machine learning.

A domain-specific language (DSL) is ``a computer programming language of limited expressiveness focused on a particular domain'' \citep{Fowler2010}. DSLs contrast with general-purpose languages, such as Python, which aim for universal applicability. When using a DSL, programmers can ``[use] the language of the domain to state the problem and to articulate solution processes'' \citep{Felleisen2015}, greatly increasing their productivity. The code they write can either look similar to the mathematical notation for the problem they are solving, such as in SPL \citep{Werk2012}, or look similar to a plain English description of the desired computation, such as in SQL \citep{Date1997}. The resemblance of SQL to plain English, for example, means that SQL imposes less of a cognitive burden on programmers than general-purpose languages do.

Machine learning models are often used in high-stakes decision making, such as in credit-scoring, housing, and hiring decisions. The outcomes of these decisions alter the life prospects of the decision-subjects, so it is important that the decisions are justified and that the model (and process used to create the model) follows the tenets of ethical machine learning. For instance, a machine learning model used across America to determine the length of criminal sentences has been found to be biased against black people, which is an instance of unethical practice of machine learning \citep{Kirchner2016}. Machine learning models tend to ``reproduce existing patterns of discrimination'' \citep{Barocas2016}, so active interventions must be taken to ensure that models account for this tendency, so that they are fair. Fortunately, there are many such interventions, including pre-processing the input data, altering the training method, and post-processing the predictions made by the model \cite{Bellamy2018,Hajian2016}. However, merely having a fair model is not enough: models must also be accountable, to allow auditors and decision-subjects to ensure that they are fair \citep{Binns2018}. Accountability is not just a property of the model, but also a property of the ``training regime'', the bundle of code and data used to develop the model. And reproducibility is required to ensure that auditors can guarantee that a deployed model is equivalent to the models that were tested for fairness, and that the code they audit is the same code that was used to create a deployed model. Ethical machine learning is an underspecified term, but it definitely includes transparency, fairness, accountability, and reproducibility.

Using this definition, we will spend the rest of the paper arguing that DSLs can help practitioners practice ethical machine learning. In Section 2, we introduce some related work, both in DSLs and tools for ethical machine learning. In Section 3, we define ethical machine learning more thoroughly, and highlight some of the key characteristics of the practice of ethical machine learning. In Section 4, we introduce Arbiter, demonstrating its syntax and core features. In Sections 5 through 8, we demonstrate how Arbiter can help practitioners achieve the four aforementioned components of the practice of ethical machine learning.

\section{Previous Work}
Machine learning practitioners have designed and implemented many DSLs to aid the practice of machine learning. For instance, TensorFlow is a DSL for expressing the matrix multiplications that are required to implement a neural network. By expressing their computations within TensorFlowâ€™s domain-specific language, users can increase the performance and legibility of their code \citep{Abadi2016}. TensorFlow is an ``internal'' DSL. Internal DSLs are implemented as a library in another language, called the ``host language'' \citep{Fowler2010}. For this reason, TensorFlow cannot make strong guarantees about the way the DSL will behave in all cases. The host language, Python, allows users to manipulate the programming language itself, and so TensorFlow code that seems to produce a fair model may only produce a fair model in certain environments. This uncertainty makes it impossible for machine learning models implemented in TensorFlow to be completely auditable and reproducible. On the other hand, an external DSL can make stronger guarantees about its outputs.

While there are no existing DSLs for ethical machine learning, there are libraries and toolkits for training fair models and evaluating the fairness of existing models. AI Fairness 360 is one prominent example \citep{Bellamy2018}. However, using AI Fairness 360 is an ad-hoc intervention, applied after the training of a model and is potentially difficult for auditors to reproduce, because the fairness-testing code may not be stored in the same place as the code that produced the model. DSLs for ethical machine learning can overcome this problem by being integral to the training regime. Furthermore, libraries such as AI Fairness 360 can easily be misused by practitioners that do not understand how to use the library properly. AI Fairness 360 contains over 5000 lines of code (as counted by \texttt{cloc}), which is large enough that users may struggle to understand it fully. AI Fairness 360 also requires that users convert their data into a somewhat rare format, creating the potential for further mistakes that could affect the integrity of the data.

Outside of machine learning, DSLs have been used before to encourage or require best practices for programming. For instance, the LangSec movement is centered around the idea that security can be increased if ``the acceptable input to a program [is] well-defined (i.e., via a grammar), as simple as possible (on the Chomsky scale of syntactic complexity), and fully validated before use'' \citep{Momot2016}. In other words, LangSec requires that software engineers use a DSL for input validation that ensures these inputs are optimized on various metrics. By explicitly limiting the set of valid inputs to a program, you can improve the security of that program. In a parallel to this idea, we will show that by explicitly limiting the set of expressible training regimes, we can encourage ethical machine learning practices.

\section{Ethical Machine Learning}
Ethical machine learning describes ethical, responsible, and thoughtful practices for the development and use of machine learning technology. Various ethicists, including Barocas \shortcite{Barocas2016} and Binns \shortcite{Binns2017}, have defended a number of principles as the required tenets of ethical machine learning; most agree that transparency, fairness, accountability, and reproducibility play important roles. Though this is not an exhaustive list of the tenets that constitute ethical machine learning, each of these principles is required for the responsible development and deployment of machine learning models.

\subsection{Transparency}
Transparency refers to openness and understandability in the training of machine learning models. A training regime is transparent if auditors and the general public can understand the it. Transparency is required because, although black-box testing can be done on the resulting models, it is much easier to audit and understand a training regime than the resulting model \citep{DeLaat2018}. Ethical machine learning must include transparency, because auditors and the general public must understand how models are being trained in order to evaluate those models. Someone without any knowledge of computer science will have trouble discerning the meaning, let alone discovering any deficiencies, in non-transparent code. Machine learning practices that increase people's ability to engage in self-directed study of an algorithm improve the transparency of said algorithm. Vitally, this ensures that people can have first-hand knowledge about a machine learning algorithm and it's function. Rather than being forced to rely solely upon the analysis of machine learning practitioners or auditors, the general public would have the option of accessing and considering information themselves.

\subsection{Fairness}
There are two definitions of fairness in machine learning: one measures equality in the distribution of outcomes by models, and the other measures fairness of the decision-making procedures that the model is employed in. For instance, an unfair distribution of outcomes deemed to be unfair could be a model that disproportionately gives women worse credit scores than men. An example of an unfair deployment of a model might be the use of a model as a judge in a courtroom. While the model may have completely equivalent outcomes between groups, meaning that it is perfectly unbiased, it may be unfair to make criminal sentencing decisions based solely on an algorithm's output. When we talk about fairness in this paper, we mean fairness in distribution of outcomes.

The deployment of unfair machine learning systems ``raises serious concerns because it risks limiting our ability to achieve the goals that we have set for ourselves and access the opportunities for which we are qualified'' \citep{Barocas2019}. However, defining fairness is difficult. AI Fairness 360 defines over 70 ways to measure fairness \citep{Bellamy2018}, and many of these metrics are contradictory, that is, an increase in one may necessitate a decrease in another \citep{Kleinberg2016}. But measuring the fairness of models is still important, both for legal compliance \citep{Barocas2016} and for ethical reasons \citep{Binns2018}. Laws such as Title VII prohibit decision-making processes that have ``disparate impact'', that is, decision-making processing that give better outcomes to people in some protected classes than others \citep{Barocas2016}. And it is unethical to construct decision-making systems that are unreasonably biased against people in marginalized groups. Because there are so many different metrics, many of them conflicting, there is no objectively "fair" or "unfair" distribution of outcomes, but it is nonetheless important for machine learning models to have imbibed metrics to assess their outcome distributions and mechanisms for appropriately adjusting them. Without these capabilities, machine learning models would be unable to recognize or amend egregiously unfair outcome distributions.

\subsection{Accountability}
Accountability in machine learning is defined by Binns \shortcite{Binns2017}: a machine learning model is accountable if it ``[provides] its decision-subjects with reasons and explanations'' for the decisions it makes. Without the capacity or obligation to provide decision-subjects with explanations, practitioners can design training regimes that produce algorithmic decision-makers that are discriminatory, arbitrary, or otherwise unjust, and conceal this fact or evade consequences for doing so. Accountability compels the defense of design decisions, and in doing so compels ethical design. For instance, a credit-scoring model may be unethically taking gender and race into account when making decisions. This mistake might go unnoticed, perpetuating inequality. However, if the model gave an explanation for its decisions, and those explanations included the race or gender of the decision-subject, then the subjects would be able to contest the legitimacy of the algorithm. In this way, accountability enables decision-subjects to advocate for themselves by providing them with grounds on which to question decisions.

\subsection{Reproducibility}
Reproducibility in machine learning requires that practitioners have the ability to reproduce models independently. Olorisade \shortcite{Olorisade2017} argues that machine learning practitioners must work to increase reproducibility, so that other people will be able to recreate any produced models in order to test them. In order for other machine learning practitioners to re-create the model to test it for unfairness, a training regime must produce the exact same model each time it is run. The requirement for reproducibility in other domains, such as biology, is that other researchers can recreate statistically similar results under the same circumstances. Reproducibility, in both scientific research and machine learning, is a safeguard against the designers of experiments or algorithms either intentionally or unknowingly introducing biases or flaws in their design. This safeguard not only ensures that the distributions of outcomes are accurate, but that the series of design decisions used to get there are justifiable and ethical.

\section{An Overview of the Language}
A domain-specific language can help machine learning practitioners uphold each of the tenets of ethical machine learning described above. The rest of this paper will examine Arbiter\footnote{A sample implementation of Arbiter can be found at \href{https://github.com/julian-zucker/arbiter}{https://github.com/julian-zucker/arbiter}. This implementation demonstrates that it would be feasible to implement Arbiter. However, the point of this paper is to explain the ethical benefits of such a language existing, not to propose the adoption of Arbiter in particular, so we choose not to focus on the implementation.}, a DSL for specifying training regimes that we claim will aid in the practice of ethical machine learning. Listing 1 is an example of a training regime, specified in Arbiter. All of the code in Listing 1 is explained in the following sections.

\begin{lstlisting}[language=Python,caption=Arbiter example.]
FROM DATA 'credit_data.csv'
TRAIN A 'decision tree'
PREDICTING 'default'
WRITE MODEL TO 'credit_score.model'
PROTECTED CLASSES 'race', 'gender', 'age'
REQUIRED FAIRNESS 'disparate impact' < 1.1
EXPLANATION 'decision_reason'
\end{lstlisting}

This language is declarative, that is, users will ``say what is required and let the system determine how to achieve it'' \citep{Roy2004}, rather than ``say[ing] how to do something'' \citep{Roy2004}, as one must in imperative programming languages. The declarative style of programming is not strictly better than the imperative style: programming languages are tools that give us notations to express computation, and ``a notation is never absolutely good ... but good only in relation to certain tasks.'' \citep{Green1989}. However, as the rest of this paper will show, the declarative paradigm and the notation implemented by Arbiter is beneficial to the task of producing transparent, fair, accountable, and reproducible machine learning models. Arbiter's syntax is based on SQL, another declarative domain-specific language. SQL, used for analysis and manipulation of tabular data, has benefited similarly from its declarative style, which makes SQL both easier to read and faster to write than equivalent imperative code.

\section{Improving Transparency}
Auditors must be able to understand the training regime that was used to develop a model; otherwise they may not be able to identify flaws or bias being introduced during the training of the model. Training regimes specified in Arbiter are understandable by people who are not machine learning practitioners. For instance, lines 1 through 4 of Listing 1 describe a fairly archetypal modeling task: given a file with comma-separated data (a ``CSV file'') containing information about historical creditor defaults, it will produce a model that can predict whether a person is likely to default on a loan. The resulting model can be employed to replace human decision-making about creditworthiness. This model will likely be biased and difficult to audit, but those concerns are handled in lines 5 through 7, which will be discussed in the sections to come.

Arbiter is more transparent than imperative programming languages are. Arbiter only needs to express the essential aspects of the machine learning model training process, while a general-purpose language must concern itself with incidental aspects of the training process, such as reading the data file and converting it into the right data format. Such incidental aspects, while strictly necessary for function, greatly complicate reading and understanding code for people who are not professional machine learning practitioners. For example, the Python code below, which is equivalent to lines 1-3 of Listing 1 is much more difficult to read. While omitted for economy of presentation, adding the code that tests for and improves fairness more than triples the length of the Python code, but adding an equivalent step to the training regime in Arbiter only requires two lines of code, nameline lines 5 and 6.

\begin{lstlisting}[language=Python,caption=Python code equivalent to lines 1-3 in Listing 1.]
with open('credit_data.csv') as credit_data:
    file_content = [line for line in csv.reader(credit_data)]
labels = [int(line[-1]) for line in file_content]
features = [[float(x) for x in line[0:-1]] for line in file_content]

from sklearn.model_selection import train_test_split
features_train, features_test, labels_train, labels_test = \
    train_test_split(features, labels)

from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier().fit(features_train, labels_train)
predictions = tree.predict(features_test)
print(f'Accuracy: {sum(predictions == labels_test) / len(features_test)}')
\end{lstlisting}

Arbiter code can ensure a higher level of transparency than libraries in general-purpose programming languages. Because Arbiter has fewer expressible computations than general-purpose languages, Arbiter requires fewer syntactic and semantic constructs, and so understanding an Arbiter program requires less effort than programs in general-purpose languages. Furthermore, even the best-designed library API can be used incorrectly, inelegantly, or incomprehensibly by code that calls the library; on the other hand, domain-specific languages can enforce consistent style and structure. Consistent style and structure, in turn, allow auditors who are less familiar with the language to nonetheless evaluate the program's behavior.

\section{Improving Fairness}
It is illegal and unethical to deploy models that are biased against people in marginalized classes. While there may be other ethical concerts with the deployment of models, based on the justice of the decision-making procedure using the model as a whole, it is better to have models that provide a more equitable distribution of outcome across marginalized classes. Arbiter allows users to make guarantees about the level of bias in any produced models. Perfect fairness of a model is impossible, but ``surely some are fairer than others'' \citep{Grant2019}, and Arbiter allows and encourages practitioners to use the fairer models. Lines 5 and 6 of Listing 1 specifies that columns of data named ``race'', ``gender'', and ``age'' represent marginalized classes, and that the resulting model must have disparate impact less than 1.1 for each of those classes. Disparate impact is a measure of unfair allocation of benefit across marginalized classes, calculated by taking the ratio of positive outcomes for the privileged class, and dividing by the ratio of positive outcomes for the marginalized class. A disparate impact of 1.0 represents benefit being perfectly evenly distributed, and  numbers larger than 1.0 represent more unfair distributions.

This de-biasing step will have to be optional, as not all datasets contain data about marginalized classes. For instance, a dataset that is collected from the actions of users on a website might not include their race, sex, or age, which would make de-biasing against those attributes impossible. However, the language could still mandate that the \texttt{PROTECTED CLASSES} and \texttt{REQUIRED FAIRNESS} directives are included anyway, but with no content after them. This way, practitioners failing to include a fairness requirement for their model will be glaringly obvious to any auditor of the code. If the absence of code to check for fairness is explicit and easily recognizable, auditors will be less likely to overlook it than if it is merely not present. While practitioners using libraries in general-purpose programming languages can write code that tests their model's fairness, that code is more likely to be buggy than the Arbiter implementation of fairness testing, as well as being less transparent than the one line of declarative code required in Arbiter.

Arbiter can use existing toolkits, such as AI Fairness 360, at multiple stages in the training process, in order to reduce bias. Practitioners will not need to know how to use these (often complex) tools, because they can simply specify the level of fairness that they want to guarantee, and let Arbiter figure out how to accomplish that. Because Arbiter is a declarative language, practitioners only have to know the desired result, not the intermediate steps required to achieve that result. Using libraries to increase the fairness of models often requires reading documentation, converting your data to other data formats, and much additional code; many practitioners likely make mistakes when using these libraries and do not increase fairness as much as they could. Arbiter abstracts away this work, allowing one group of developers to write the code that interfaces with libraries that improve fairness, saving all other developers from having to duplicate that work. Reducing the amount of work required to design a fair training regime will increase the number of developers who do so.

\section{Improving Accountability}
Accountability is a necessary piece of ethical machine learning. To be accountable, a model must ``provide its decision-subjects with reasons and explanations'' \citep{Binns2017} for the decisions it makes. Arbiter can ensure that models are explainable: because Arbiter holds the responsibility for training the model, the language can ensure that the model is trained in a way that includes explanations. For example, it could use TED \citep{Hind2019}, allowing users to specify a column of the input dataset that contains the explanation for the decisions made in the input dataset, as shown in line 7 of Listing 1. Then, the model produced can return not only a prediction but also an explanation of the reason for that prediction. Importantly, machine learning practitioners will not have to implement TED themselves, or know how it works -- they will merely have to specify in their training regime that they want explanations for each prediction, and it will happen. This is one of the key benefits of Arbiter's declarative style -- practitioners do not need to understand how to use myriad different libraries and tools, and the interactions between them. One coherent tool can incorporate many of the aspects of ethical machine learning.

Furthermore, Arbiter could automatically produce tools for exploring and explaining the models that it generates. For instance, a tool that allows practitioners to apply the model to various data points to see what decision is made, and what explanation for the decision is given. Additional tooling that supports transparency can be produced automatically, without requiring any time or effort from the practitioner training the model, similarly to how the team behind TensorFlow created TensorBoard, a tool that automatically visualizes neural networks being trained in TensorFlow. Similarly to how reducing the amount of work required to design a fair training regime will promote fairness, reducing the amount of work required to create an accountable model will promote accountability.

\section{Improving Reproducibility}
Training machine learning models is typically a non-deterministic process involving stochastic methods. These stochastic methods ``seed'' a random number generator with the current time, preventing future auditors from replicating the exact training process. But merely choosing a static ``seed'' value is not enough to guarantee reproducibility, as there are many other ways that software can be non-deterministic \citep{Maste2017}, and the compilers or interpreters that execute the code may contain vulnerabilities \citep{Thompson1984}. So, auditors re-running a training regime will almost certainly end up with a different model. And once the two models are not exactly byte-for-byte identical, it becomes very difficult to identify the difference in behavior between the two \citep{Perry2014}.

However, a language like Arbiter can guarantee reproducibility. For example, by using a hash of the data in the training specification as the seed value, Arbiter can guarantee that every execution of the same training specification will be using the same random seed. Arbiter can be implemented to produce the same model for any input training regime, unlike Python libraries, which cannot guarantee reproducibility. Even if a Python library is itself reproducible, users may misuse it or write non-reproducible code that calls into it, resulting in non-deterministic training regimes. Training must be reproducible, and this can only be accomplished through a DSL. On the other hand, Arbiter has full control over how the model that the user has declared will be trained, so it can ensure that it uses only reproducible libraries, and that those libraries are composed in a way that remains fully reproducible. Furthermore, in order to guarantee reproducibility in practice, many different inputs to Arbiter can be tested, and reproducibility can be demonstrated for each of those inputs. This is unlikely to happen in standard software engineering, where time constraints and fiscal limitations often prevent practitioners from testing their code for reproducibility. Arbiter can ensure reproducibility by forbidding non-reproducible computations to take place in its limited, domain-specific language.

\section{Conclusion}
We presented Arbiter, a DSL that can aid in the practice of ethical machine learning. Arbiter improves transparency, mandates fairness, enables accountability, and guarantees reproducibility in the machine learning training process. While Arbiter is not a silver bullet, it lays the groundwork for a programming-language-based approach to ethical machine learning. And, by establishing some desirable features of a DSL for ethical machine learning, we have provided a basis for further work in DSL design for ethical programming in general.

% References and End of Paper
% These lines must be placed at the end of your paper
\bibliographystyle{ACM-Reference-Format}
\bibliography{arbiter.bib}
\end{document}
